{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f163e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Add src/ to path\n",
    "sys.path.append('C:/Users/acer/OneDrive/Desktop/ML/ML_Hackathon_Hackman/src')\n",
    "from hmm_model import HMM\n",
    "\n",
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "INDEX = {c: i for i, c in enumerate(ALPHABET)}\n",
    "MAX_LIVES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c823b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for corpus at: C:\\Users\\acer\\OneDrive\\Desktop\\ML\\ML_Hackathon_Hackman\\data\\corpus.txt\n",
      "Total lines in file: 50000\n",
      "Valid words after filtering: 49979\n",
      "Loading HMM from file...\n",
      "Max word length: 24\n",
      "Loaded 49,979 words and HMM.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# SECTION 2 – LOAD DATA & HMM (WITH DEBUG)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "# --- 1. Load corpus.txt (with debug) ---\n",
    "corpus_path = '../data/corpus.txt'\n",
    "print(f\"Looking for corpus at: {os.path.abspath(corpus_path)}\")\n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    raise FileNotFoundError(f\"corpus.txt not found at {corpus_path}\")\n",
    "\n",
    "raw_lines = []\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines in file: {len(raw_lines)}\")\n",
    "\n",
    "# Filter valid words\n",
    "words = []\n",
    "for line in raw_lines:\n",
    "    w = line.strip().lower()\n",
    "    if w and w.isalpha():\n",
    "        words.append(w)\n",
    "\n",
    "print(f\"Valid words after filtering: {len(words)}\")\n",
    "\n",
    "if len(words) == 0:\n",
    "    raise ValueError(\"No valid words found in corpus.txt! \"\n",
    "                     \"It must contain English words (letters only), one per line.\")\n",
    "\n",
    "# --- 2. Load or create HMM ---\n",
    "hmm_path = '../models/hmm.pkl'\n",
    "if os.path.exists(hmm_path):\n",
    "    print(\"Loading HMM from file...\")\n",
    "    with open(hmm_path, 'rb') as f:\n",
    "        hmm = pickle.load(f)\n",
    "else:\n",
    "    print(\"No HMM found – training from scratch...\")\n",
    "    from src.hmm_model import HMM\n",
    "    hmm = HMM(words)\n",
    "    hmm.save(hmm_path)\n",
    "    print(\"HMM trained and saved!\")\n",
    "\n",
    "# --- 3. Final setup ---\n",
    "MAX_LEN = max(len(w) for w in words)\n",
    "print(f\"Max word length: {MAX_LEN}\")\n",
    "print(f\"Loaded {len(words):,} words and HMM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3a4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus...\n",
      "Loaded 0 words. Training HMM...\n",
      "✅ HMM trained & saved to: C:/Users/acer/OneDrive/Desktop/ML/ML_Hackathon_Hackman\\models\\hmm.pkl\n",
      "File size: 5,839 bytes\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# TRAIN & SAVE REAL HMM (Run ONCE) - FIXED PATHS\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Fix for any working directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path (absolute)\n",
    "project_root = r'C:/Users/acer/OneDrive/Desktop/ML/ML_Hackathon_Hackman'\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "from hmm_model import HMM\n",
    "\n",
    "# Ensure models folder exists\n",
    "models_dir = os.path.join(project_root, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load corpus\n",
    "corpus_path = os.path.join(project_root, 'data', 'corpus.txt')\n",
    "if not os.path.exists(corpus_path):\n",
    "    raise FileNotFoundError(f\"Missing {corpus_path}\")\n",
    "\n",
    "print(\"Loading corpus...\")\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    words = [line.strip().lower() for line in f if line.strip() and line.isalpha()]\n",
    "\n",
    "print(f\"Loaded {len(words):,} words. Training HMM...\")\n",
    "\n",
    "# Train & save\n",
    "hmm = HMM(words)\n",
    "model_path = os.path.join(models_dir, 'hmm.pkl')\n",
    "hmm.save(model_path)\n",
    "\n",
    "print(f\"HMM trained & saved to: {model_path}\")\n",
    "print(f\"File size: {os.path.getsize(model_path):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3c74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
